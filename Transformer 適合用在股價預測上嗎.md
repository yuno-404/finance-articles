---
title: Transformer é©åˆç”¨åœ¨è‚¡åƒ¹é æ¸¬ä¸Šå—?

---

{%preview https://www.sciencedirect.com/science/article/abs/pii/S0957417423019267 %}
Series decomposition Transformer with period-correlation for stock market index prediction
é€™ç¯‡è«–æ–‡å¾ˆçŸ­ï¼Œä¸»è¦å°±æ˜¯æ¨¡å‹æ¶æ§‹è·Ÿæ¼”ç®—æ³•æ€éº¼è·‘çš„ã€‚
æ¨¡å‹å‰µæ–°ï¼šæå‡ºäº†ä¸€ç¨®åç‚º SDTP çš„æ–°å‹æ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚
æ ¸å¿ƒæ©Ÿåˆ¶ï¼šè©²æ¨¡å‹çµåˆäº†å…©å¤§é—œéµæŠ€è¡“ï¼š
* åºåˆ—åˆ†è§£å±¤ (Series Decomposition Layer)ã€‚
* é€±æœŸç›¸é—œæ©Ÿåˆ¶ (Period-Correlation Mechanism)ã€‚

ç›®çš„ï¼šé€éé€™äº›æ©Ÿåˆ¶ä¾†æ•æ‰æ™‚é–“åºåˆ—è³‡æ–™ä¸­å›ºæœ‰çš„é€±æœŸæ€§ (Inherent Periodicity) ä»¥åŠåºåˆ—ä¹‹é–“çš„é—œè¯æ€§ (Relation)ã€‚

ä¸‹é¢æ˜¯æˆ‘åšçš„PPTï¼Œæ‡¶å¾—è½‰æˆæ–‡å­—äº†
![image](https://hackmd.io/_uploads/HJPR6mPB-g.png)
![image](https://hackmd.io/_uploads/SJrgauwB-e.png)
![image](https://hackmd.io/_uploads/ByMJ6_vB-l.png)
![image](https://hackmd.io/_uploads/ryWfTuDB-g.png)
![image](https://hackmd.io/_uploads/HJ17p_PH-l.png)
![image](https://hackmd.io/_uploads/ry6NTdPS-g.png)
![image](https://hackmd.io/_uploads/ryurT_vH-l.png)
![image](https://hackmd.io/_uploads/H1B86uPS-x.png)
![image](https://hackmd.io/_uploads/SJe_a_wrbe.png)
![image](https://hackmd.io/_uploads/Sk1KpdwrZl.png)
![image](https://hackmd.io/_uploads/HkOtaOPSZe.png)
![image](https://hackmd.io/_uploads/H1G9p_PSZg.png)
![image](https://hackmd.io/_uploads/ry59TOvHWe.png)
![image](https://hackmd.io/_uploads/H1Es6uwB-e.png)


----
è«–æ–‡æ²’çµ¦åƒæ•¸è·Ÿæ¨¡å‹åŸå§‹ç¢¼ï¼Œæˆ‘è·‘S&Pçš„æ•¸æ“šåšä¸å‡ºä¾†è·Ÿè«–æ–‡ä¸€æ¨£çš„çµæœã€‚
ä½†ä»–é€™å€‹æƒ³æ³•ç®—æ˜¯æœ‰è¶£ï¼Œå¾ŒçºŒå¯ä»¥é‡å°é€™å€‹æ¨¡å‹åšäº›èª¿æ•´
#### è¶¨å‹¢å®šç¾©å•é¡Œ
* ä½¿ç”¨ AvgPool (ç§»å‹•å¹³å‡) ä¾†å®šç¾©ã€Œè¶¨å‹¢ ($X_t$)ã€
    * ç§»å‹•å¹³å‡ç·šåœ¨æœ¬è³ªä¸Šæ˜¯æ»¯å¾ŒæŒ‡æ¨™
    * é€™å°è‡´æ¨¡å‹è¨“ç·´æ™‚ï¼ŒEncoder èªå®šã€Œè¶¨å‹¢ã€é‚„åœ¨å¾€ä¸‹ï¼Œä½†å¯¦éš›ä¸Šåƒ¹æ ¼å·²ç¶“åè½‰å‘ä¸Š
* ä¿®æ­£
    * å°‡ AvgPool æ›¿æ›ç‚º Conv1d å±¤
        * è®“ç¥ç¶“ç¶²è·¯è‡ªå·±å­¸ç¿’ä»€éº¼æ¨£çš„å¹³æ»‘æ›²ç·šæœ€é©åˆä»£è¡¨ç•¶ä¸‹çš„è¶¨å‹¢
    * æ”¹ç”¨å¤šé …å¼æ“¬åˆ
        * ä½¿ç”¨ Autoformer ä¸­çš„åšæ³•ï¼Œåˆ©ç”¨å¤šé …å¼å›æ­¸ä¾†æå–è¶¨å‹¢
#### Decoder åˆå§‹åŒ–å•é¡Œ
* è«–æ–‡åœ¨ Decoder è¼¸å…¥ç«¯ï¼Œå°‡æœªä¾†çš„æ³¢å‹•éƒ¨åˆ† ($X_{des}$) å¡«è£œç‚º 0
    * é€™å‡è¨­äº†å¸‚å ´ç¸½æ˜¯å‚¾å‘æ–¼ã€Œç¬é–“å›æ­¸å‡ç·šã€ï¼Œå°æ–¼ å‹•èƒ½å¼·å‹ çš„è¶¨å‹¢è‚¡ï¼Œæœƒå°è‡´é æ¸¬å€¼åš´é‡ä½ä¼°æ³¢å‹•å¹…åº¦
* ä¿®æ­£
    * ç·šæ€§å¤–æ¨ 
        * ä¸è¦è£œ 0ï¼Œè€Œæ˜¯è¨ˆç®—éå»å¹¾é»çš„æ–œç‡ï¼Œå°‡æ³¢å‹•å€¼ç·šæ€§å»¶ä¼¸å¡«å…¥
    * æœ€å¾Œå€¼å¡«å……
        * æœ€ç°¡å–®çš„ä½œæ³•ï¼Œå‡è¨­æ³¢å‹•ç¶­æŒåœ¨æœ€å¾Œä¸€åˆ»çš„å¼·åº¦
    * 
#### æ­£è¦åŒ–æ–¹æ³•çš„å•é¡Œ
* è«–æ–‡ä½¿ç”¨å…¨åŸŸçµ±è¨ˆæ•¸æ“šåš Z-score
    * é‡‘èæ™‚é–“åºåˆ—å…·æœ‰ éå¹³ç©©æ€§ (Non-stationarity)
    * 2020 å¹´çš„æ³¢å‹•ç‡ (Std) å¯èƒ½è·Ÿ 2024 å¹´å®Œå…¨ä¸åŒï¼Œæœƒå°è‡´ã€Œæ¦‚å¿µé£„ç§» (Concept Drift)ã€
* ä¿®æ­£
    * å¼•å…¥ RevIN (å„ªå…ˆè™•ç†é€™å€‹ï¼Œé€™å€‹æ–¹æ³•æœ‰æ–™)
        * å°æ¯ä¸€å€‹ Batch çš„è¼¸å…¥ç¨ç«‹åšæ­£è¦åŒ–ï¼Œæ¨¡å‹åªå­¸ã€Œå½¢ç‹€ã€ï¼Œè¼¸å‡ºå¾Œå†æŠŠè©² Batch çš„å¹³å‡å€¼èˆ‡æ¨™æº–å·®ä¹˜å›å»ã€‚é€™èƒ½å¤§å¹…æå‡æ¨¡å‹å°ä¸åŒå¸‚å ´ç’°å¢ƒçš„é©æ‡‰åŠ›

#### æå¤±å‡½æ•¸çš„é¸æ“‡ (å„ªå…ˆè™•ç†ï¼Œæˆ‘è¦ºå¾—é€™è£¡ä»–ç¢ºå¯¦é¸äº†ä¸€å€‹æ»¿çˆ›çš„ loss function)
* ä½¿ç”¨ MSE (å‡æ–¹èª¤å·®) ä½œç‚º Loss Function
    * MSE å°æ–¼ã€Œæ–¹å‘ã€ä¸æ•æ„Ÿï¼Œä¸”å°ç•°å¸¸å€¼éæ–¼æ•æ„Ÿ
    * åœ¨äº¤æ˜“ä¸­ï¼Œé æ¸¬æ¼²è·Œæ–¹å‘ (Direction) å¾€å¾€æ¯”é æ¸¬çµ•å°æ•¸å€¼æ›´é‡è¦
* ä¿®æ­£ 
    * åŠ å…¥æ–¹å‘æ€§æ‡²ï¼Œçµ¦äºˆé¡å¤–çš„é‡ç½°
    * ä¸è¦åªé æ¸¬ä¸€å€‹åƒ¹æ ¼ï¼Œè€Œæ˜¯é æ¸¬åƒ¹æ ¼çš„å€é–“




åº•ä¸‹æ˜¯å˜—è©¦å»åšè«–æ–‡å¾©ç¾çš„ç¨‹å¼ç¢¼,é€²è¡Œäº†å…©è™•ä¿®æ­£ã€‚
## ç¨‹å¼ç¯„ä¾‹
### æ”¹å‹•å¯å­¸ç¿’åºåˆ—åˆ†è§£ (Learnable Series Decomposition)
$$W' = \text{Softmax}(W_{learnable})$$
$$X_{trend} = X * W'$$
$$X_{seasonal} = X - X_{trend}$$
ç‚ºä»€éº¼è¦ç”¨ Softmaxï¼Ÿ
ç‚ºäº†ä¿æŒã€Œè¶¨å‹¢ã€çš„ç‰©ç†æ„ç¾©ï¼Œæ¿¾æ³¢å™¨çš„æ¬Šé‡ç¸½å’Œå¿…é ˆç‚º 1 ($\sum W'_i = 1$)ã€‚å¦‚æœä¸åšé™åˆ¶ï¼Œå·ç©å¾Œçš„æ•¸å€¼æœƒç„¡é™æ”¾å¤§æˆ–ç¸®å°ï¼Œå°è‡´ $X - X_{trend}$ å¤±å»æ„ç¾©ã€‚
### æ”¹å‹•å‹•èƒ½å¤–æ¨åˆå§‹åŒ– (Linear Extrapolation Initialization)
ä»¤ Encoder è¼¸å‡ºçš„æœ€å¾Œä¸€å€‹æ™‚é–“é»ç‚º $t$ï¼Œå€’æ•¸ç¬¬äºŒå€‹æ™‚é–“é»ç‚º $t-1$ã€‚
å°æ–¼æœªä¾†ç¬¬ $\tau$ å€‹é æ¸¬é» ($1 \le \tau \le L_{pred}$)ï¼Œåˆå§‹åŒ–çš„é æ¸¬å€¼ $\hat{X}_{t+\tau}$ è¨ˆç®—å¦‚ä¸‹ï¼š
$$\text{Slope} = X_t - X_{t-1}$$
$$\hat{X}_{t+\tau} = X_t + \text{Slope} \times \tau$$
$X_t$ æ˜¯ Encoder è¼¸å‡ºçš„æœ€å¾Œä¸€ç­†æ•¸æ“šã€‚
$\text{Slope}$ ä»£è¡¨ç•¶ä¸‹çš„ç¬æ™‚é€Ÿåº¦ã€‚
æœ€å¾Œçš„æ”¹å–„å¾Œçš„çµæœæœ‰æ¯”åŸä¾†æˆ‘é€²è¡Œè¤‡ç¾çš„ç¨‹å¼ç¢¼è¡¨ç¾å¥½ï¼Œä½†é‚„æ˜¯æ²’æœ‰æ¯”è«–æ–‡ä¸­çš„é«˜ï¼Œè«–æ–‡å°s&pçš„æº–ç¢ºåº¦é«˜é”0.98ï¼Œæˆ‘åªèƒ½åšåˆ°0.975ï¼ŒçœŸçš„ä¸æ›‰å¾—åƒæ•¸è©²æ€éº¼èª¿äº†

## ç¨‹å¼ç¢¼
```
# ==========================================
# 1. å¥—ä»¶å®‰è£èˆ‡åŒ¯å…¥
# ==========================================
# !pip install yfinance scikit-learn matplotlib
import pandas as pd
import jax
import jax.numpy as jnp
from jax import random, grad, jit, vmap
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import time

print(f"JAX ç‰ˆæœ¬: {jax.__version__}")
print(f"è£ç½®: {jax.devices()}")
print()

# ==========================================
# 2. çœŸå¯¦æ•¸æ“šè¼‰å…¥èˆ‡è™•ç† (Real World Data)
# ==========================================

# è¨­å®šåƒæ•¸ (è«–æ–‡ Table 4 æŒ‡å®š S&P 500 å€é–“)
TICKER = "^GSPC"
START_DATE = "2010-01-04"
END_DATE = "2018-12-28"

print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ {TICKER} æ•¸æ“š...")
# --- å»ºæ§‹è«–æ–‡ Section 5.1 å®šç¾©çš„ 8 å€‹ç‰¹å¾µ ---
# 1. Volume
# 2. Turnover (ä¼°ç®—å€¼: Volume * Closeï¼Œå›  Yahoo ä¸æä¾›æŒ‡æ•¸æˆäº¤é¡)
# 3. Change (Close - Prev Close)
# 4. Change rate ((Close - Prev Close) / Prev Close)
# 5. High
# 6. Low
# 7. Open
# 8. Close

df = yf.download(TICKER, start=START_DATE, end=END_DATE)
if isinstance(df.columns, pd.MultiIndex):
    df.columns = df.columns.get_level_values(0)
# ç‚ºäº†è¨ˆç®— Change å’Œ Change Rateï¼Œæˆ‘å€‘éœ€è¦ shift
df['Prev_Close'] = df['Close'].shift(1)

# è¨ˆç®—ç‰¹å¾µ
df['Change'] = df['Close'] - df['Prev_Close']
df['Change_Rate'] = (df['Close'] - df['Prev_Close']) / df['Prev_Close']
df['Turnover'] = df['Volume'] * df['Close'] # ä¼°ç®—

# ç§»é™¤ç¬¬ä¸€ç­† (å› ç‚º shift ç”¢ç”Ÿ NaN)
df = df.dropna()

# é¸å–ä¸¦æ’åºç‰¹å¾µ (ä¾ç…§ Table 2 é †åº)
feature_cols = [
    'Volume', 'Turnover', 'Change', 'Change_Rate',
    'High', 'Low', 'Open', 'Close'
]

# ç¢ºä¿åªå–é€™äº›æ¬„ä½çš„å€¼
data_raw = df[feature_cols].values

print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼Œè³‡æ–™å½¢ç‹€: {data_raw.shape}")
print(f"   åŒ…å«ç‰¹å¾µ: {feature_cols}")


print(f"âœ… ä¸‹è¼‰å®Œæˆï¼Œç¸½ç­†æ•¸: {len(data_raw)}")

# --- æ•¸æ“šæ­£è¦åŒ– (Z-score) ---
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_raw)

# --- è£½ä½œ Time Series Dataset ---
# ä¾æ“šè«–æ–‡åƒæ•¸: Lag=5 (SEQ_LEN), Predict=1 (PRED_LEN)
SEQ_LEN = 5
PRED_LEN = 1

X_data, Y_data = [], []
for i in range(len(data_scaled) - SEQ_LEN - PRED_LEN+1):
    X_data.append(data_scaled[i : i+SEQ_LEN])      
    Y_data.append(data_scaled[i+SEQ_LEN : i+SEQ_LEN+PRED_LEN]) 

X = np.array(X_data).astype(np.float32)  # Shape: [N, 5, 8]
Y = np.array(Y_data).astype(np.float32)  # Shape: [N, 1, 8]

# åˆ‡åˆ†è¨“ç·´é›† (80%) èˆ‡æ¸¬è©¦é›† (20%)
train_size = int(len(X) * 0.8)
X_train, Y_train = X[:train_size], Y[:train_size]
X_test, Y_test = X[train_size:], Y[train_size:]

print(f"ğŸ“Š æ•¸æ“šé›†æº–å‚™å®Œæˆ: è¨“ç·´é›† {len(X_train)} ç­†, æ¸¬è©¦é›† {len(X_test)} ç­†")
print()

# ==========================================
# 3. SDTP æ¨¡å‹å®šç¾© (JAX ç‰ˆæœ¬ - æ”¹é€²ç‰ˆ)
# ==========================================

# åƒæ•¸è¨­å®š
INPUT_DIM = 8
D_MODEL = 64
N_HEADS = 4
N_ENCODER_LAYERS = 2
N_DECODER_LAYERS = 2
D_FF = 256
KERNEL_SIZE = 3
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.001

# ============================================================
# æ ¸å¿ƒçµ„ä»¶ (æ”¹é€²ç‰ˆï¼šå¯å­¸ç¿’åˆ†è§£ + å‹•èƒ½å¤–æ¨)
# ============================================================

def linear_extrapolation(seq, pred_len):
    """
    [NEW] ç·šæ€§å‹•èƒ½å¤–æ¨
    seq: (batch, seq_len, features)
    """
    # å–æœ€å¾Œå…©é»è¨ˆç®— "é€Ÿåº¦" (Slope)
    last_val = seq[:, -1:, :]
    prev_val = seq[:, -2:-1, :]

    # æ–œç‡ = (ç¾åœ¨ - ä¸Šä¸€åˆ»)
    slope = last_val - prev_val

    # ç”¢ç”Ÿæœªä¾†çš„æ™‚é–“æ­¥é•· [1, 2, ..., pred_len]
    time_steps = jnp.arange(1, pred_len + 1).reshape(1, -1, 1)

    # æœªä¾†é æ¸¬ = æœ€å¾Œä¸€é» + æ–œç‡ * æ™‚é–“
    pred = last_val + slope * time_steps
    return pred

def learnable_moving_average(x, kernel_weights):
    """
    [NEW] ä½¿ç”¨å¯å­¸ç¿’æ¬Šé‡çš„ç§»å‹•å¹³å‡
    x: (seq_len,)
    kernel_weights: (kernel_size,)
    """
    kernel_size = kernel_weights.shape[0]
    pad_size = kernel_size // 2

    # Padding
    x_padded = jnp.pad(x, pad_size, mode='edge')

    # é—œéµï¼šä½¿ç”¨ softmax ç¢ºä¿æ¬Šé‡ç¸½å’Œç‚º 1 (ä¿æŒè¶¨å‹¢çš„æ•¸å€¼è¦æ¨¡)
    w_norm = jax.nn.softmax(kernel_weights)

    # å·ç©
    trend = jnp.convolve(x_padded, w_norm, mode='valid')

    # è™•ç†å¶æ•¸ kernel å¯èƒ½å°è‡´çš„é•·åº¦èª¤å·® (é˜²å‘†)
    return trend[:x.shape[0]]

def series_decomposition(x, kernel_weights):
    """
    [MODIFIED] åºåˆ—åˆ†è§£: Trend + Seasonal
    ç¾åœ¨æ¥å— kernel_weights è€Œä¸æ˜¯å›ºå®šçš„ kernel_size
    """
    batch, seq_len, features = x.shape

    def process_single(x_single):
        # x_single: (seq_len, features)
        # vmap over features (axis 1)
        return vmap(learnable_moving_average, in_axes=(1, None), out_axes=1)(
            x_single, kernel_weights
        )

    trend = vmap(process_single)(x)
    seasonal = x - trend
    return seasonal, trend

def period_correlation(params, query, key, value, mask=None):
    """Period-Correlation Mechanism"""
    batch_size, seq_len, d_model = query.shape
    d_k = d_model // N_HEADS

    Q = query @ params['W_q']
    K = key @ params['W_k']
    V = value @ params['W_v']

    Q = Q.reshape(batch_size, seq_len, N_HEADS, d_k).transpose(0, 2, 1, 3)
    K = K.reshape(batch_size, seq_len, N_HEADS, d_k).transpose(0, 2, 1, 3)
    V = V.reshape(batch_size, seq_len, N_HEADS, d_k).transpose(0, 2, 1, 3)

    scores = (Q @ jnp.swapaxes(K, -2, -1)) / jnp.sqrt(d_k)

    if mask is not None:
        scores = jnp.where(mask, scores, -1e9)

    attn = jax.nn.softmax(scores, axis=-1)
    output = attn @ V

    output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)
    output = output @ params['W_o']

    return output

@jit
def feed_forward(params, x):
    """Feed Forward Network"""
    hidden = jax.nn.relu(x @ params['W1'] + params['b1'])
    output = hidden @ params['W2'] + params['b2']
    return output

@jit
def layer_norm(x, gamma, beta, eps=1e-5):
    """Layer Normalization"""
    mean = jnp.mean(x, axis=-1, keepdims=True)
    var = jnp.var(x, axis=-1, keepdims=True)
    return gamma * (x - mean) / jnp.sqrt(var + eps) + beta

def encoder_layer_forward(params, x, kernel_weights):
    """Encoder Layer (Accepts kernel_weights)"""
    # Period-Correlation
    attn_out = period_correlation(params['attn'], x, x, x)
    x = x + attn_out

    # Decomposition
    seasonal, _ = series_decomposition(x, kernel_weights)
    seasonal = layer_norm(seasonal, params['norm1_gamma'], params['norm1_beta'])

    # Feed Forward
    ffn_out = feed_forward(params['ffn'], seasonal)
    seasonal = seasonal + ffn_out

    # Decomposition
    seasonal_out, _ = series_decomposition(seasonal, kernel_weights)
    seasonal_out = layer_norm(seasonal_out, params['norm2_gamma'], params['norm2_beta'])

    return seasonal_out

def decoder_layer_forward(params, seasonal_input, trend_input, enc_output, kernel_weights):
    """Decoder Layer (Accepts kernel_weights)"""
    trend_accum = trend_input

    # Self-Attention
    self_attn = period_correlation(params['self_attn'],
                                   seasonal_input, seasonal_input, seasonal_input)
    seasonal = seasonal_input + self_attn
    seasonal, trend1 = series_decomposition(seasonal, kernel_weights)
    trend_accum = trend_accum + trend1 @ params['W_trend1']

    # Cross-Attention
    cross_attn = period_correlation(params['cross_attn'],
                                   seasonal, enc_output, enc_output)
    seasonal = seasonal + cross_attn
    seasonal, trend2 = series_decomposition(seasonal, kernel_weights)
    trend_accum = trend_accum + trend2 @ params['W_trend2']

    # Feed Forward
    ffn_out = feed_forward(params['ffn'], seasonal)
    seasonal = seasonal + ffn_out
    seasonal_out, trend3 = series_decomposition(seasonal, kernel_weights)
    trend_out = trend_accum + trend3 @ params['W_trend3']

    return seasonal_out, trend_out

# ============================================================
# æ¨¡å‹åˆå§‹åŒ–èˆ‡å‰å‘å‚³æ’­
# ============================================================

def init_sdtp_params(key,input_dim):
    """åˆå§‹åŒ– SDTP æ¨¡å‹åƒæ•¸"""
    keys = random.split(key, 25)

    params = {
        'input_proj': random.normal(keys[0], (input_dim, D_MODEL)) * 0.02, # <--- æ”¹é€™è£¡
        'output_proj': random.normal(keys[1], (D_MODEL, input_dim)) * 0.02, # <--- æ”¹é€™è£¡

        # [NEW] å¯å­¸ç¿’åˆ†è§£å·ç©æ ¸ (Learnable Kernel)
        # åˆå§‹åŒ–ç‚º 1/k (å¹³å‡å€¼) ä¸¦åŠ ä¸Šå¾®å°é›œè¨Šä»¥ä¾¿æ¢¯åº¦ä¸‹é™é–‹å§‹é‹ä½œ
        'decomp_kernel': jnp.ones(KERNEL_SIZE) / KERNEL_SIZE + \
                         random.normal(keys[2], (KERNEL_SIZE,)) * 0.001,

        'encoder': [],
        'decoder': []
    }

    # Encoder
    for i in range(N_ENCODER_LAYERS):
        key_i = keys[3 + i]
        k1, k2, k3, k4, k5, k6 = random.split(key_i, 6)

        encoder_params = {
            'attn': {
                'W_q': random.normal(k1, (D_MODEL, D_MODEL)) * 0.02,
                'W_k': random.normal(k2, (D_MODEL, D_MODEL)) * 0.02,
                'W_v': random.normal(k3, (D_MODEL, D_MODEL)) * 0.02,
                'W_o': random.normal(k4, (D_MODEL, D_MODEL)) * 0.02,
            },
            'ffn': {
                'W1': random.normal(k5, (D_MODEL, D_FF)) * 0.02,
                'b1': jnp.zeros(D_FF),
                'W2': random.normal(k6, (D_FF, D_MODEL)) * 0.02,
                'b2': jnp.zeros(D_MODEL),
            },
            'norm1_gamma': jnp.ones(D_MODEL),
            'norm1_beta': jnp.zeros(D_MODEL),
            'norm2_gamma': jnp.ones(D_MODEL),
            'norm2_beta': jnp.zeros(D_MODEL),
        }
        params['encoder'].append(encoder_params)

    # Decoder
    for i in range(N_DECODER_LAYERS):
        key_i = keys[3 + N_ENCODER_LAYERS + i]
        k1, k2, k3, k4, k5, k6, k7, k8, k9, k10, k11, k12, k13 = random.split(key_i, 13)

        decoder_params = {
            'self_attn': {
                'W_q': random.normal(k1, (D_MODEL, D_MODEL)) * 0.02,
                'W_k': random.normal(k2, (D_MODEL, D_MODEL)) * 0.02,
                'W_v': random.normal(k3, (D_MODEL, D_MODEL)) * 0.02,
                'W_o': random.normal(k4, (D_MODEL, D_MODEL)) * 0.02,
            },
            'cross_attn': {
                'W_q': random.normal(k5, (D_MODEL, D_MODEL)) * 0.02,
                'W_k': random.normal(k6, (D_MODEL, D_MODEL)) * 0.02,
                'W_v': random.normal(k7, (D_MODEL, D_MODEL)) * 0.02,
                'W_o': random.normal(k8, (D_MODEL, D_MODEL)) * 0.02,
            },
            'ffn': {
                'W1': random.normal(k9, (D_MODEL, D_FF)) * 0.02,
                'b1': jnp.zeros(D_FF),
                'W2': random.normal(k10, (D_FF, D_MODEL)) * 0.02,
                'b2': jnp.zeros(D_MODEL),
            },
            'W_trend1': random.normal(k11, (D_MODEL, D_MODEL)) * 0.01,
            'W_trend2': random.normal(k12, (D_MODEL, D_MODEL)) * 0.01,
            'W_trend3': random.normal(k13, (D_MODEL, D_MODEL)) * 0.01,
        }
        params['decoder'].append(decoder_params)

    return params

def sdtp_forward(params, x, kernel_size=3):
    """SDTP å‰å‘å‚³æ’­ - åŒ…å«å‹•èƒ½å¤–æ¨èˆ‡å¯å­¸ç¿’åˆ†è§£"""
    batch_size = x.shape[0]

    # æå–å­¸ç¿’åˆ°çš„ kernel weights
    kernel_weights = params['decomp_kernel']

    # Input Embedding + Decomposition
    x_embed = x @ params['input_proj']
    enc_seasonal, enc_trend = series_decomposition(x_embed, kernel_weights)

    # Encoder
    for enc_params in params['encoder']:
        enc_seasonal = encoder_layer_forward(enc_params, enc_seasonal, kernel_weights)

    # ============================================================
    # [MODIFIED] Decoder Initialization
    # ============================================================

    # æ³¢å‹•è»Œï¼šæ­·å² (Token) + é æ¸¬ (Prediction)
    # Token éƒ¨åˆ†
    dec_seasonal_token = enc_seasonal[:, -(SEQ_LEN-PRED_LEN):, :]
    # Prediction éƒ¨åˆ†ï¼šä½¿ç”¨ [NEW] å‹•èƒ½å¤–æ¨ è€Œä¸æ˜¯è£œ 0
    # æˆ‘å€‘å°æ³¢å‹•é …åšè¼•å¾®çš„å¤–æ¨ï¼ˆæˆ–è€…ä¿æŒ0ï¼‰ï¼Œé€™è£¡ç¤ºç¯„å‹•èƒ½å¤–æ¨
    dec_seasonal_pred = linear_extrapolation(enc_seasonal, PRED_LEN)

    dec_seasonal = jnp.concatenate([dec_seasonal_token, dec_seasonal_pred], axis=1)

    # è¶¨å‹¢è»Œï¼šæ­·å² (Token) + é æ¸¬ (Prediction)
    dec_trend_token = enc_trend[:, -(SEQ_LEN-PRED_LEN):, :]
    # Prediction éƒ¨åˆ†ï¼šä½¿ç”¨ [NEW] å‹•èƒ½å¤–æ¨ è€Œä¸æ˜¯è£œ Mean
    dec_trend_pred = linear_extrapolation(enc_trend, PRED_LEN)

    dec_trend = jnp.concatenate([dec_trend_token, dec_trend_pred], axis=1)

    # Decoder
    for dec_params in params['decoder']:
        dec_seasonal, dec_trend = decoder_layer_forward(
            dec_params, dec_seasonal, dec_trend, enc_seasonal, kernel_weights
        )

    # Output
    final_seasonal = dec_seasonal[:, -PRED_LEN:, :]
    final_trend = dec_trend[:, -PRED_LEN:, :]
    
    # ã€ä¿®æ­£ã€‘å°‡å…©è€…åœ¨éš±è—å±¤(64ç¶­)å…ˆç›¸åŠ ï¼Œå†é€šé output_proj è½‰å› (8ç¶­)
    # é€™æ˜¯æœ€ç©©å¥çš„åšæ³•ï¼Œç¢ºä¿ Trend å’Œ Seasonal éƒ½ç¶“éæ­£ç¢ºçš„æ¬Šé‡è½‰æ›
    predictions = (final_seasonal + final_trend) @ params['output_proj']
    
    return predictions

# ============================================================
# æå¤±å‡½æ•¸èˆ‡å„ªåŒ–å™¨
# ============================================================
@jit
def direction_weighted_loss(params, x, y_true, kernel_size, lambda_dir=5.0):
    """
    çµåˆ MSE èˆ‡ æ–¹å‘æ€§æ‡²ç½°
    lambda_dir: æ–¹å‘æ‡²ç½°ä¿‚æ•¸ï¼Œè¨­è¶Šå¤§æ¨¡å‹è¶Šåœ¨æ„æ¼²è·Œæ–¹å‘
    """
    # 1. å–å¾—é æ¸¬å€¼
    y_pred = sdtp_forward(params, x, kernel_size)
    
    # -------------------------------------------------------
    # æŠ€å·§ï¼šæˆ‘å€‘ä¸åªçœ‹æ•¸å€¼ï¼Œæ›´çœ‹ã€Œè®ŠåŒ–é‡ (Delta)ã€
    # -------------------------------------------------------
    # å–å¾—è¼¸å…¥åºåˆ—çš„æœ€å¾Œä¸€é» (Last Known Value)
    # x shape: (Batch, Seq, Features), Close is index 7
    last_close = x[:, -1:, 7:8] 
    
    # è¨ˆç®—çœŸå¯¦çš„æ¼²è·Œ (Delta True)
    # y_true shape: (Batch, Pred, Features)
    delta_true = y_true[:, :, 7:8] - last_close
    
    # è¨ˆç®—é æ¸¬çš„æ¼²è·Œ (Delta Pred)
    delta_pred = y_pred[:, :, 7:8] - last_close
    
    # 2. åŸºç¤ MSE Loss
    mse = jnp.mean((y_pred - y_true) ** 2)
    
    # 3. æ–¹å‘æ€§ Loss (Directional Loss)
    # å¦‚æœ sign(delta_true) != sign(delta_pred)ï¼Œå‰‡çµ¦äºˆæ‡²ç½°
    # jnp.sign å›å‚³ -1, 0, 1
    true_sign = jnp.sign(delta_true)
    pred_sign = jnp.sign(delta_pred)
    
    # åªæœ‰ç•¶æ–¹å‘ç›¸åæ™‚ (ç›¸ä¹˜ < 0)ï¼Œæ‰æœƒæœ‰å€¼
    direction_error = jnp.where(true_sign * pred_sign < 0, jnp.abs(delta_true - delta_pred), 0.0)
    dir_loss = jnp.mean(direction_error)
    
    # 4. ç¸½ Loss
    total_loss = mse + lambda_dir * dir_loss
    
    return total_loss

def mse_loss(params, x, y_true, kernel_size):
    """MSE Loss - kernel_size åƒ…ä½œç‚º padding åƒè€ƒï¼Œå¯¦éš›é‹ç®—ä½¿ç”¨ params['decomp_kernel']"""
    y_pred = sdtp_forward(params, x, kernel_size)
    return jnp.mean((y_pred - y_true) ** 2)

# ç·¨è­¯æ¢¯åº¦å‡½æ•¸
loss_and_grad = jit(jax.value_and_grad(direction_weighted_loss), static_argnums=(3,))

# ç°¡å–®çš„ Adam å„ªåŒ–å™¨å¯¦ä½œ
def init_adam_state(params):
    """åˆå§‹åŒ– Adam å„ªåŒ–å™¨ç‹€æ…‹"""
    m = jax.tree.map(lambda p: jnp.zeros_like(p), params)
    v = jax.tree.map(lambda p: jnp.zeros_like(p), params)
    return {'m': m, 'v': v, 't': 0}

@jit
def adam_update(params, grads, opt_state, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
    """Adam å„ªåŒ–å™¨æ›´æ–°æ­¥é©Ÿ"""
    t = opt_state['t'] + 1
    m = jax.tree.map(lambda m_i, g: beta1 * m_i + (1 - beta1) * g, opt_state['m'], grads)
    v = jax.tree.map(lambda v_i, g: beta2 * v_i + (1 - beta2) * g**2, opt_state['v'], grads)

    m_hat = jax.tree.map(lambda m_i: m_i / (1 - beta1**t), m)
    v_hat = jax.tree.map(lambda v_i: v_i / (1 - beta2**t), v)

    params = jax.tree.map(
        lambda p, m_i, v_i: p - lr * m_i / (jnp.sqrt(v_i) + eps),
        params, m_hat, v_hat
    )

    return params, {'m': m, 'v': v, 't': t}

# ==========================================
# 4. è¨“ç·´è¿´åœˆ
# ==========================================

print("ğŸš€ é–‹å§‹è¨“ç·´ SDTP æ¨¡å‹ (JAX æ”¹é€²ç‰ˆ: å‹•èƒ½å¤–æ¨ + å¯å­¸ç¿’åˆ†è§£)...")
print(f"åƒæ•¸é…ç½®: d_model={D_MODEL}, n_heads={N_HEADS}, layers={N_ENCODER_LAYERS}")
print()

# åˆå§‹åŒ–æ¨¡å‹
key = random.PRNGKey(42)
params = init_sdtp_params(key, INPUT_DIM)

# è¨ˆç®—åƒæ•¸é‡
n_params = sum(x.size for x in jax.tree.leaves(params))
print(f"ç¸½åƒæ•¸é‡: {n_params:,}")
print()

# åˆå§‹åŒ–å„ªåŒ–å™¨
opt_state = init_adam_state(params)

# Warm-up (JIT ç·¨è­¯)
print("Warm-up (JIT ç·¨è­¯)...")
x_sample = jnp.array(X_train[:BATCH_SIZE])
y_sample = jnp.array(Y_train[:BATCH_SIZE])

start = time.time()
loss_val, grads = loss_and_grad(params, x_sample, y_sample, KERNEL_SIZE)
jax.tree.map(lambda x: x.block_until_ready(), grads)
t_warmup = time.time() - start
print(f"ç·¨è­¯æ™‚é–“: {t_warmup:.4f}s")
print(f"åˆå§‹æå¤±: {loss_val:.6f}")
print()

# è¨“ç·´å¾ªç’°
loss_history = []
start_time = time.time()

for epoch in range(EPOCHS):
    # æ‰“äº‚è¨“ç·´æ•¸æ“š
    n_train = len(X_train)
    perm = np.random.permutation(n_train)

    epoch_losses = []

    # æ‰¹æ¬¡è¨“ç·´
    for i in range(0, n_train, BATCH_SIZE):
        batch_idx = perm[i:i+BATCH_SIZE]
        if len(batch_idx) < BATCH_SIZE:
            continue

        x_batch = jnp.array(X_train[batch_idx])
        y_batch = jnp.array(Y_train[batch_idx])

        # è¨ˆç®—æå¤±å’Œæ¢¯åº¦
        loss_val, grads = loss_and_grad(params, x_batch, y_batch, KERNEL_SIZE)

        # Adam æ›´æ–°
        params, opt_state = adam_update(params, grads, opt_state, lr=LEARNING_RATE)

        epoch_losses.append(float(loss_val))

    avg_loss = np.mean(epoch_losses)
    loss_history.append(avg_loss)

    if (epoch + 1) % 10 == 0:
        elapsed = time.time() - start_time
        print(f"Epoch {epoch+1:3d}/{EPOCHS}, Loss: {avg_loss:.6f}, Time: {elapsed:.2f}s")

total_time = time.time() - start_time
print()
print(f"âœ… è¨“ç·´å®Œæˆï¼ç¸½æ™‚é–“: {total_time:.2f}s")
print()

# ==========================================
# 5. è©•ä¼°èˆ‡æŒ‡æ¨™è¨ˆç®— (ä¿®æ­£ç¶­åº¦éŒ¯èª¤ç‰ˆ)
# ==========================================

print("ğŸ” æ­£åœ¨é€²è¡Œæ¸¬è©¦é›†è©•ä¼° (Target: Close Price)...")

# ç·¨è­¯æ¨è«–å‡½æ•¸
forward_jit = jit(sdtp_forward, static_argnums=(2,))

preds_scaled_all = []
trues_scaled_all = []

# åˆ†æ‰¹é æ¸¬
test_batch_size = 32
for i in range(0, len(X_test), test_batch_size):
    x_batch = jnp.array(X_test[i:i+test_batch_size])
    y_batch = Y_test[i:i+test_batch_size]

    pred = forward_jit(params, x_batch, KERNEL_SIZE).block_until_ready()

    # ã€é—œéµä¿®æ­£ 1ã€‘æå–æ‰€æœ‰ 8 å€‹ç‰¹å¾µ (Volume...Close)ï¼Œè€Œä¸æ˜¯åªæœ‰ Close
    # pred shape: (Batch, 1, 8) -> å–å‡º (Batch, 8)
    preds_scaled_all.extend(pred[:, 0, :]) 
    trues_scaled_all.extend(y_batch[:, 0, :])

# è½‰ç‚º NumPy é™£åˆ—ï¼Œå½¢ç‹€æ‡‰ç‚º (N, 8)
preds_scaled_all = np.array(preds_scaled_all)
trues_scaled_all = np.array(trues_scaled_all)

# ã€é—œéµä¿®æ­£ 2ã€‘åæ­£è¦åŒ– (ç¾åœ¨è¼¸å…¥æ˜¯ 8 ç¶­ï¼ŒScaler æ‰èƒ½æ­£å¸¸å·¥ä½œ)
preds_real_all = scaler.inverse_transform(preds_scaled_all)
trues_real_all = scaler.inverse_transform(trues_scaled_all)

# ã€é—œéµä¿®æ­£ 3ã€‘åæ­£è¦åŒ–å¾Œï¼Œå†å–®ç¨å–å‡º Close Price (ç¬¬ 7 æ¬„)
# feature_cols = ['Volume', ..., 'Close']
close_idx = 7 

preds_close = preds_real_all[:, close_idx]
trues_close = trues_real_all[:, close_idx]

# è¨ˆç®—æŒ‡æ¨™
mse = mean_squared_error(trues_close, preds_close)
rmse = np.sqrt(mse)
mae = mean_absolute_error(trues_close, preds_close)
r2 = r2_score(trues_close, preds_close)
mape = np.mean(np.abs((trues_close - preds_close) / trues_close)) * 100

print("-" * 40)
print(f"ğŸ† æ¸¬è©¦é›†è©•ä¼°çµæœ (S&P 500 Close Price):")
print(f"RMSE (å‡æ–¹æ ¹èª¤å·®):   {rmse:.4f}")
print(f"MAE  (å¹³å‡çµ•å°èª¤å·®): {mae:.4f}")
print(f"MAPE (ç™¾åˆ†æ¯”èª¤å·®):   {mape:.4f}%")
print(f"RÂ²   (æ±ºå®šä¿‚æ•¸):     {r2:.4f}")
print("-" * 40)
print()

# ==========================================
# 6. è¦–è¦ºåŒ–
# ==========================================

# é æ¸¬çµæœå°æ¯”
plt.figure(figsize=(12, 6))
plot_len = 100
# ä½¿ç”¨ä¿®æ­£å¾Œçš„è®Šæ•¸åç¨±: trues_close, preds_close
plt.plot(trues_close[-plot_len:], label='Ground Truth (Real Price)', color='green', linewidth=2)
plt.plot(preds_close[-plot_len:], label='SDTP Prediction (JAX)', color='red', linestyle='--', linewidth=2)
plt.title(f'SDTP Improved Prediction (Last {plot_len} Days)', fontsize=14)
plt.ylabel('Price (USD)', fontsize=12)
plt.xlabel('Days', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Training Loss æ›²ç·š
plt.figure(figsize=(8, 5))
plt.plot(loss_history, linewidth=2)
plt.title('Training Loss (MSE)', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ==========================================
# 7. æ¨è«–é€Ÿåº¦æ¸¬è©¦
# ==========================================

print("âš¡ æ¸¬è©¦æ¨è«–é€Ÿåº¦...")
x_bench = jnp.array(X_test[:BATCH_SIZE])

# Warm-up
_ = forward_jit(params, x_bench, KERNEL_SIZE).block_until_ready()

# æ¸¬è©¦
start = time.time()
n_iterations = 1000
for _ in range(n_iterations):
    _ = forward_jit(params, x_bench, KERNEL_SIZE).block_until_ready()
t_infer = time.time() - start

print(f"1000 æ¬¡æ¨è«–æ™‚é–“: {t_infer:.4f}s")
print(f"å¹³å‡æ¯æ¬¡: {t_infer/n_iterations*1000:.2f}ms")
print(f"æ¯ç§’å¯è™•ç†: {BATCH_SIZE * n_iterations / t_infer:.0f} æ¨£æœ¬")
```
